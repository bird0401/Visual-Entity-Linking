{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scraping_from_wikimedia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNyIhORB8uowyhEftGDX2t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bird0401/Instance_level_recognition/blob/main/scraping_from_wikimedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install fake_useragent"
      ],
      "metadata": {
        "id": "oZIXqxJJFepd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vES-efu-GJxz"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "from fake_useragent import UserAgent\n",
        "import pathlib\n",
        "import time\n",
        "from requests.exceptions import Timeout\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "import traceback    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ua = UserAgent()\n",
        "header = {'user-agent':ua.chrome}\n",
        "\n",
        "wikimedia_url = 'https://commons.wikimedia.org'"
      ],
      "metadata": {
        "id": "hEOB6vQX88ct"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ToAbsURL(related_url = '/wiki/Category'):\n",
        "  base_url = wikimedia_url\n",
        "  return base_url+related_url\n",
        "  \n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1))\n",
        "def Fetch(url):\n",
        "  \"\"\"\n",
        "  add below handlings to normal request\n",
        "  - 3 times retry\n",
        "  - 1s sleep\n",
        "  - exception handling\n",
        "  \"\"\"\n",
        "\n",
        "  # when 200<=res.status_code<300 execute code in try statement\n",
        "  # this time, 200 will return because of success of get request is 200\n",
        "  try: \n",
        "    res = requests.get(url, headers=header, timeout=10)\n",
        "  except Timeout:\n",
        "    print('Timeout has been raised.')\n",
        "    return None\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    return None\n",
        "  \n",
        "  time.sleep(1) \n",
        "  return res\n",
        "\n",
        "def ExtractNextPageURL(url, text=\"next page\"):\n",
        "  res = Fetch(url)\n",
        "  soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "  try:\n",
        "    t=soup.find(text=text)\n",
        "    if t: return ToAbsURL(related_url = t.parent.attrs['href'])\n",
        "  except: \n",
        "    traceback.print_exc()\n",
        "    return None"
      ],
      "metadata": {
        "id": "9LMcHMJuwZy8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ExtractEntityURLs(category):\n",
        "  entity_urls=[]\n",
        "  entity_list_page_url=ToAbsURL(related_url = f'/wiki/Category:{category}')\n",
        "\n",
        "  while entity_list_page_url:\n",
        "    res = Fetch(entity_list_page_url)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "    \n",
        "    try:\n",
        "      elems=soup.find_all(class_=\"CategoryTreeItem\")\n",
        "      for elem in elems:\n",
        "        entity_url=elem.find('a').attrs['href']\n",
        "        yield ToAbsURL(related_url = entity_url)\n",
        "    except:\n",
        "      traceback.print_exc()\n",
        "\n",
        "    print(entity_list_page_url)\n",
        "    entity_list_page_url=ExtractNextPageURL(entity_list_page_url)"
      ],
      "metadata": {
        "id": "95AysRVhicJw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ExtractEntityID(entity_url):\n",
        "  try: \n",
        "    res = Fetch(entity_url)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\") \n",
        "    wikidata_url=soup.find(href=re.compile(\"^https://www.wikidata.org/wiki/Q\")).attrs[\"href\"]\n",
        "    wikidata_id=pathlib.Path(wikidata_url).stem\n",
        "    return wikidata_id\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    return None\n",
        "\n",
        "def MakeEntityImgDir(id):\n",
        "  img_path = \"./imgs/\" + id\n",
        "  os.makedirs(img_path,exist_ok=True)\n",
        "  return img_path"
      ],
      "metadata": {
        "id": "nN8GSRaDirnm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ExtractImageURL(img_page_url):\n",
        "      try: \n",
        "        res = Fetch(img_page_url)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "        l=soup.find(class_=\"fullImageLink\")\n",
        "        if l: \n",
        "          img_url = l.a.attrs['href']\n",
        "          return img_url\n",
        "        else: \n",
        "          print(\"can't extract image URL\") # for example, in the case that the file is mp3\n",
        "      except: \n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def ExtractImageURLs(entity_img_list_page_url):\n",
        "  \"\"\"\n",
        "  Image Page is the page which contains image, description, bottons, etc.\n",
        "  after extract Image Page URL, I should extract image url from this Page \n",
        "  \"\"\"\n",
        "  while entity_img_list_page_url:\n",
        "    res = Fetch(entity_img_list_page_url)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "    try:\n",
        "      image_classes=soup.find_all(class_=\"galleryfilename galleryfilename-truncate\")\n",
        "      for image_class in image_classes:\n",
        "        img_page_url=ToAbsURL(image_class.attrs['href'])\n",
        "        img_url = ExtractImageURL(img_page_url)\n",
        "        if img_url: yield img_url\n",
        "    except:\n",
        "      traceback.print_exc()\n",
        "    entity_img_list_page_url=ExtractNextPageURL(entity_img_list_page_url)\n",
        "\n",
        "def DownloadImage(url, file_path):\n",
        "  res=Fetch(url)\n",
        "  if res: \n",
        "    with open(file_path, \"wb\") as f: f.write(res.content)\n",
        "\n",
        "def DownloadImages(entity_url):\n",
        "  wikidata_id = ExtractEntityID(entity_url)\n",
        "  if not wikidata_id: return None\n",
        "  img_dir_path = MakeEntityImgDir(wikidata_id)\n",
        "  for i, img_url in enumerate(ExtractImageURLs(entity_url)):\n",
        "    filename = 'image_' + str(i).zfill(3) + '.jpg'\n",
        "    img_file_path = os.path.join(img_dir_path, filename)\n",
        "    print(f\"URL: {img_url}\")\n",
        "    print(f\"Path: {img_file_path}\")\n",
        "    DownloadImage(url=img_url, file_path=img_file_path)"
      ],
      "metadata": {
        "id": "OW286LmBhSxn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entity_urls=ExtractEntityURLs(category='Dog_breeds_by_name')\n",
        "# for e in entity_urls:\n",
        "#   print(e)"
      ],
      "metadata": {
        "id": "7gpft0Hz5QWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # we can iterate only one time because entity_urls is iterator\n",
        "entity_urls=ExtractEntityURLs(category='Dog_breeds_by_name')\n",
        "for entity_url in entity_urls:\n",
        "  DownloadImages(entity_url)"
      ],
      "metadata": {
        "id": "wLGucqYnz6Mh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}